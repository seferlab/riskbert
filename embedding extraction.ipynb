{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"embedding extraction.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMFZl4sAtHSvMHdjQi+elEm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"dk_SvVepbx2d"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yxPRx5P9biGH"},"outputs":[],"source":["!pip install transformers\n","!pip install torch\n","!pip install scikit-learn"]},{"cell_type":"code","source":["from transformers import BertTokenizer\n","#the model constructed previously\n","tokenizer = BertTokenizer.from_pretrained('/content/drive/MyDrive/modeller/output_bert_base', truncation=True)  "],"metadata":{"id":"wlAWdnCfb0XH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["for the Kogan Levin collection"],"metadata":{"id":"zK5BInfpiFg7"}},{"cell_type":"code","source":["import pandas as pd\n","#dataframe with text year volatility values\n","df = pd.read_csv(\"/content/drive/MyDrive/my_data.csv\")\n","df.columns = ['Text', 'year', 'real','before']"],"metadata":{"id":"AK3DjS9kb6EA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["for the data after 2006"],"metadata":{"id":"mEeSBZ-2iHz4"}},{"cell_type":"code","source":["import pandas as pd\n","df = pd.read_csv(\"/content/drive/MyDrive/later2007data.csv\")\n","df.columns = ['Text', 'year', 'real','before','docname']"],"metadata":{"id":"gnyh9zlTiDls"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# for the collection of Kogan and Levin"],"metadata":{"id":"YxQBCbXscF_h"}},{"cell_type":"code","source":["docs = \"/content/drive/MyDrive/docs2/\"\n","vols = \"/content/drive/MyDrive/vols/vols\""],"metadata":{"id":"PMxDIP7hcERm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertTokenizer, BertModel\n","import torch\n","from csv import writer\n","import glob\n","\n","def embed_bert(model_name,tokenizer_name):#(df,model_name,tokenizer_name):\n","  tokenizer = BertTokenizer.from_pretrained(tokenizer_name)\n","  model = BertModel.from_pretrained(model_name)\n","  lengths =[]\n","  for i in range(1996,2007):\n","    print(\"year\",i)\n","    path = docs+str(i)+\".mda\"\n","    # All files \n","    for file in glob.glob(path+\"/*.mda\"):\n","      f = open(file,encoding=\"latin\")\n","      text= f.read()\n","      with open('res_data.csv', 'a') as f_object:\n","        # Pass this file object to csv.writer()\n","        # and get a writer object\n","        writer_object = writer(f_object)\n","\n","        # Pass the list as an argument into\n","        # the writerow()\n","        c = list(embedding(text,tokenizer,model))\n","        List = [file,c[0],c[1]]\n","        writer_object.writerow(List)\n","\n","        #Close the file object\n","        f_object.close()\n","      f.close()\n"],"metadata":{"id":"qyT41mGScRQl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# for the data after 2006"],"metadata":{"id":"ErtiVt62gvBO"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertModel\n","import torch\n","from csv import writer\n","import glob\n","import csv\n","from csv import writer\n","def isNaN(string):\n","    return string != string\n","def embed_bert(model_name,tokenizer_name):#(df,model_name,tokenizer_name):\n","  tokenizer = BertTokenizer.from_pretrained(tokenizer_name)\n","  model = BertModel.from_pretrained(model_name)\n","  #f = open(file,'a')\n","  #df2 = pd.DataFrame([list(embedding(df[\"Text\"][0],tokenizer,model))],columns=['mean','std'])\n","  for i in range(24175,len(df)):\n","    with open('/content/drive/MyDrive/result_data.csv', 'a') as f_object:\n","      print('row_number:',i)\n","      # Pass this file object to csv.writer()\n","      # and get a writer object\n","      writer_object = writer(f_object)\n","      text = df.iloc[i]['Text']\n","      if not isNaN(text):\n","        real = df.iloc[i]['real']\n","        year = df.iloc[i]['year']\n","        before = df.iloc[i]['before']\n","        name = df.iloc[i]['docname']\n","        # Pass the list as an argument into\n","        # the writerow()\n","        c,d = embedding(text,tokenizer,model)\n","        List = [name,c,d,real,before,year] #docname v1 v2 real before name\n","        writer_object.writerow(List)\n","\n","        #Close the file object\n","        f_object.close()\n","      else:\n","        f_object.close()\n","  #f.write(str(embedding(df[\"Text\"][i],tokenizer,model)))\n","  #df2= df['Text'].apply(lambda x: embedding(x,tokenizer,model))\n","  #f.close()\n","  #return df2"],"metadata":{"id":"Ta2bKeG4gyuG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# embedding extarction files"],"metadata":{"id":"LRFWml33dCeP"}},{"cell_type":"code","source":["def into_the_model(model,my_input_ids,my_attention_mask):\n","  my_input_ids = torch.stack(tuple(torch.from_numpy(my_input_ids)))\n","  my_attention_mask = torch.stack(tuple(torch.from_numpy(my_attention_mask)))\n","  #my_token_type_ids = torch.stack(tuple(torch.from_numpy(my_token_type_ids)))\n","\n","  input_dict = {\n","      'input_ids': my_input_ids.long(),\n","      #'token_type_ids':my_token_type_ids.int(),\n","      'attention_mask': my_attention_mask.int()\n","  }\n","  outputs = model(**input_dict)\n","  last_hidden_states = outputs.last_hidden_state\n","  return last_hidden_states #torch.Tensor type\n","def embedding(text, tokenizer,model):\n","  inputs = tokenizer.encode_plus(text,'padding=True','truncation=True', return_tensors=\"pt\")\n","  if inputs['input_ids'].size()[1]>510:\n","    input_ids=inputs['input_ids'][0].split(510)\n","    #token_type_ids =inputs['token_type_ids'][0].split(510)\n","    attention_mask=inputs['attention_mask'][0].split(510)\n","    length = len(input_ids)\n","    count = 0\n","    #lastest = torch.Tensor()\n","    lastest = np.empty([0])\n","    my_input_ids =np.empty([0])\n","    print(len(my_input_ids))\n","    my_attention_mask=np.empty([0])\n","    #my_token_type_ids=np.empty([0])\n","    while count<length:\n","      pad_len = 512 - input_ids[count].shape[0]\n","      # check if tensor length satisfies required chunk size\n","      if pad_len > 0:\n","          # if padding length is more than 0, we must add padding\n","          if count>0:\n","            my_input_ids=np.vstack((my_input_ids, torch.cat([\n","                input_ids[count], torch.Tensor([0] * pad_len)] # torch.Tensor([0] * pad_len)\n","            ).detach().numpy()))\n","            print(len(my_input_ids))\n","            my_attention_mask=np.vstack((my_attention_mask, torch.cat([\n","                attention_mask[count], torch.Tensor([0] * pad_len)]\n","            ).detach().numpy() ) )\n","            #my_token_type_ids=np.vstack((my_token_type_ids,torch.cat([\n","            #    token_type_ids[count], torch.Tensor([1] * pad_len)]).detach().numpy()\n","            #))\n","          else:\n","            my_input_ids=torch.cat([\n","                input_ids[count], torch.Tensor([0] * pad_len)] # torch.Tensor([0] * pad_len)\n","            ).detach().numpy()\n","            print(len(my_input_ids))\n","            my_attention_mask= torch.cat([\n","                attention_mask[count], torch.Tensor([0] * pad_len)]\n","            ).detach().numpy()\n","            #my_token_type_ids=torch.cat([\n","            #    token_type_ids[count], torch.Tensor([1] * pad_len)]).detach().numpy()\n","\n","      count+=1\n","    if len(my_input_ids)>30:\n","      lastest = np.empty([0])\n","      b = int(len(my_input_ids)/30)\n","      parts_input_ids = np.split(my_input_ids[:30*b],b)\n","      parts_attention_mask = np.split(my_attention_mask[:30*b],b)\n","      if len(my_input_ids)%30!=0:\n","        parts_input_ids.append(my_input_ids[30*b:])\n","        parts_attention_mask.append(my_attention_mask[30*b:])\n","        b = b+1\n","      for i in range(b):\n","        print(i)\n","        if i>0:\n","          lastest =np.vstack((lastest ,into_the_model(model,parts_input_ids[i],parts_attention_mask[i]).detach().numpy()  ))\n","        else: \n","          lastest = into_the_model(model,parts_input_ids[i],parts_attention_mask[i]).detach().numpy()\n","    else:\n","      lastest =into_the_model(model,my_input_ids,my_attention_mask)\n","    lastest = lastest.reshape(lastest.shape[0]*lastest.shape[1],768)\n","  else:\n","    outputs = model(**inputs)\n","    lastest = outputs.last_hidden_state.detach().numpy()\n","  mean = np.mean(lastest, axis =0)\n","  std = np.std(lastest, axis =0)\n","  return mean, std\n","  #return torch.mean(lastest[0], 0).detach().numpy(),torch.std(lastest[0], 0).detach().numpy()\n"],"metadata":{"id":"b0Ln16y9cS77"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embed_bert('/content/drive/MyDrive/modeller/output_bert_base','/content/drive/MyDrive/modeller/output_bert_base') \n","# for the bert embedings, embed_bert function was \n","# embed_bert('bert-base-uncased','bert-base-uncased')"],"metadata":{"id":"UC-UbER3cWY9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# modifications in functions to find closer words"],"metadata":{"id":"AOAJ2QcddwFE"}},{"cell_type":"code","source":["import string\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","stop_words = list(set(stopwords.words('english')))\n","stop_words.append('could')\n","stop_words.append('although')\n","#stop_words.append('code')\n","stop_words.append('despite')\n","stop_words.append('within')\n","stop_words.append('also')\n","stop_words.append('pad')\n","stop_words.append('cls')\n","def seven_up(lastest,input_ids,vector,tokenizer):\n","  tuple_list = []\n","  ret_list =[]\n","  for index in range(len(lastest)):\n","    temp = lastest[index][:] - vector\n","    sum_sq = np.dot(temp.T, temp)\n","    tuple_list.append((input_ids[index],sum_sq))\n","  tuple_list.sort(key=lambda x:x[1])\n","  words = [tokenizer.decode(x[0]) for x in tuple_list]\n","  words = [x.lower() for x in words]\n","  words = [x for x in words if x!=\"[pad]\" and x not in string.punctuation and x not in stop_words and not x.isnumeric()]\n","  print(\"there are \",len(words))\n","  return words"],"metadata":{"id":"tSdzmorBd9s1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def into_the_model(model,my_input_ids,my_attention_mask):\n","  my_input_ids = torch.stack(tuple(torch.from_numpy(my_input_ids)))\n","  my_attention_mask = torch.stack(tuple(torch.from_numpy(my_attention_mask)))\n","  #my_token_type_ids = torch.stack(tuple(torch.from_numpy(my_token_type_ids)))\n","\n","  input_dict = {\n","      'input_ids': my_input_ids.long(),\n","      #'token_type_ids':my_token_type_ids.int(),\n","      'attention_mask': my_attention_mask.int()\n","  }\n","  outputs = model(**input_dict)\n","  last_hidden_states = outputs.last_hidden_state\n","  return last_hidden_states #torch.Tensor type\n","def embedding(text, tokenizer,model):\n","  ##text = text.lower()\n","  ##text = text.translate()\n","  ##words = [x for x in words if x!=\"[pad]\" and x not in string.punctuation and x not in stop_words and not x.isnumeric()]\n","  inputs = tokenizer.encode_plus(text,'padding=True','truncation=True', return_tensors=\"pt\")\n","  if inputs['input_ids'].size()[1]>510:\n","    input_ids=inputs['input_ids'][0].split(510)\n","    #token_type_ids =inputs['token_type_ids'][0].split(510)\n","    attention_mask=inputs['attention_mask'][0].split(510)\n","    length = len(input_ids)\n","    count = 0\n","    #lastest = torch.Tensor()\n","    lastest = np.empty([0])\n","    our_input_ids = np.empty([0])\n","    my_input_ids =np.empty([0])\n","    print(len(my_input_ids))\n","    my_attention_mask=np.empty([0])\n","    #my_token_type_ids=np.empty([0])\n","    while count<length:\n","      pad_len = 512 - input_ids[count].shape[0]\n","      # check if tensor length satisfies required chunk size\n","      if pad_len > 0:\n","          # if padding length is more than 0, we must add padding\n","          if count>0:\n","            my_input_ids=np.vstack((my_input_ids, torch.cat([\n","                input_ids[count], torch.Tensor([0] * pad_len)] # torch.Tensor([0] * pad_len)\n","            ).detach().numpy()))\n","            #print(len(my_input_ids))\n","            our_input_ids =np.vstack((our_input_ids,torch.cat([\n","                input_ids[count], torch.Tensor([0] * pad_len)]).detach().numpy().reshape( 512,1)) )\n","            #print(len(our_input_ids))\n","            my_attention_mask=np.vstack((my_attention_mask, torch.cat([\n","                attention_mask[count], torch.Tensor([0] * pad_len)]\n","            ).detach().numpy() ) )\n","            #my_token_type_ids=np.vstack((my_token_type_ids,torch.cat([\n","            #    token_type_ids[count], torch.Tensor([1] * pad_len)]).detach().numpy()\n","            #))\n","          else:\n","            my_input_ids=torch.cat([\n","                input_ids[count], torch.Tensor([0] * pad_len)] # torch.Tensor([0] * pad_len)\n","            ).detach().numpy()\n","            our_input_ids =my_input_ids.reshape(len(my_input_ids),1)\n","            #print(len(my_input_ids))\n","            my_attention_mask= torch.cat([\n","                attention_mask[count], torch.Tensor([0] * pad_len)]\n","            ).detach().numpy()\n","            #my_token_type_ids=torch.cat([\n","            #    token_type_ids[count], torch.Tensor([1] * pad_len)]).detach().numpy()\n","\n","      count+=1\n","    if len(my_input_ids)>30:\n","      lastest = np.empty([0])\n","      b = int(len(my_input_ids)/30)\n","      parts_input_ids = np.split(my_input_ids[:30*b],b)\n","      parts_attention_mask = np.split(my_attention_mask[:30*b],b)\n","      if len(my_input_ids)%30!=0:\n","        parts_input_ids.append(my_input_ids[30*b:])\n","        parts_attention_mask.append(my_attention_mask[30*b:])\n","        b = b+1\n","      for i in range(b):\n","        #print(i)\n","        if i>0:\n","          lastest =np.vstack((lastest ,into_the_model(model,parts_input_ids[i],parts_attention_mask[i]).detach().numpy()  ))\n","          \n","        else: \n","          lastest = into_the_model(model,parts_input_ids[i],parts_attention_mask[i]).detach().numpy()\n","          \n","    else:\n","      lastest =into_the_model(model,my_input_ids,my_attention_mask).detach().numpy()\n","      \n","    lastest = lastest.reshape(lastest.shape[0]*lastest.shape[1],768)\n","  else:\n","    outputs = model(**inputs)\n","    lastest = outputs.last_hidden_state.detach().numpy()\n","  #my_input_ids = my_input_ids.reshape(my_input_ids.shape[1] *my_input_ids.shape[0],1)\n","  print(\"lastest shape\",lastest.shape,\"input_ids shape \", our_input_ids.shape)\n","  print(len(lastest[0]))\n","  mean = np.mean(lastest, axis =0)\n","  std = np.std(lastest, axis =0)\n","  wm = seven_up(lastest,our_input_ids,mean,tokenizer)\n","  ws = seven_up(lastest,our_input_ids,std,tokenizer)\n","  #print(wm)\n","  print(len(wm))\n","  #return 0\n","  return wm[-100:],ws[-100:]"],"metadata":{"id":"tkX7Ln8ueJZm"},"execution_count":null,"outputs":[]}]}