{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ML Text regression.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPmHFsFH2QlF1kAZWSpKKi8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"x-Zn-2Wzhpm8"},"outputs":[],"source":["!pip install transformers\n","!pip install torch\n","!pip install scikit-learn"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"bo068cJ2iW01"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","used = pd.read_csv(\"/content/drive/MyDrive/result_data.csv\") #/content/drive/MyDrive/ml/finbert/res_data_all.csv\n","used.columns = ['name','v1','v2','real','before','year']"],"metadata":{"id":"ZYeOIAUyidSS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#for the data after 2006 , volatility values are not logged in the later2007.csv file!!!!\n","used.loc[used['year']>2006,'real'] = used.loc[df4['year']>2006,'real'].apply(lambda x: math.log(x) )\n","used.loc[used['year']>2006,'before'] = used.loc[df4['year']>2006,'before'].apply(lambda x: math.log(x) )"],"metadata":{"id":"2N1zpoZJmhLg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#NaN = np.nan\n","import numpy as np\n","used['features']=\"\"\n","for i in range(len(used)):  \n","  count = 0\n","  my_list = used.loc[i,['v1']].v1.split()\n","  other_list = used.loc[i,['v2']].v2.split()\n","  if '[' in my_list:\n","    my_list.remove('[')\n","  if '[' in other_list:\n","    other_list.remove('[')\n","  if ']' in my_list:\n","    my_list.remove(']')\n","  if ']' in other_list:\n","    other_list.remove(']')\n","  for x in my_list:\n","    if my_list[count]!='[':\n","      if count==0:\n","        my_list[count]=my_list[count][1:]\n","        other_list[count]=other_list[count][1:]\n","      if count==len(my_list)-1:\n","        my_list[count]=my_list[count][:-1]\n","        other_list[count]=other_list[count][:-1]\n","      my_list[count] = float(my_list[count])\n","      other_list[count] = float(other_list[count])\n","      count+=1\n","      \n","  my_list=np.array(my_list)\n","  other_list=np.array(other_list)\n","  overall =np.hstack((my_list,other_list)) \n","  #overall =np.hstack((my_list,other_list,used.loc[i,['before']].before)) --> if previous value will be added for the kogan levin data\n","\n","  used.at[i,'features']= overall"],"metadata":{"id":"PNuMbFxzixGD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# tf and tf-idf features"],"metadata":{"id":"n0qdvYVti-uz"}},{"cell_type":"code","source":["df2 =used"],"metadata":{"id":"7jQqic69jL45"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["tf features:"],"metadata":{"id":"HRP_HKRmjP2e"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","  \n","# Create a Vectorizer Object\n","tf_idf_vec_smooth = TfidfVectorizer(use_idf=False,  \n","                        #smooth_idf=True,  \n","                        ngram_range=(1,1),stop_words='english',max_features=2000)\n"," \n"," \n","tf_idf_data = tf_idf_vec_smooth.fit_transform(df2['Text'].to_numpy())"],"metadata":{"id":"JJoy8bTnjGDH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df2['tf'] = tf_idf_data.toarray().tolist()#arr.toarray().tolist()"],"metadata":{"id":"jWw8d8k5jdx4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["tf-idf features"],"metadata":{"id":"7PLeohwojRk4"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","  \n","#with smooth\n","tf_idf_vec_smooth = TfidfVectorizer(use_idf=True,  \n","                        smooth_idf=True,  \n","                        ngram_range=(1,1),stop_words='english',max_features=2000)\n"," \n"," \n","tf_idf_data = tf_idf_vec_smooth.fit_transform(df2['Text'].to_numpy())\n"],"metadata":{"id":"TP9HeCAVjTqW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df2['tfidf'] = tf_idf_data.toarray().tolist()#arr.toarray().tolist()"],"metadata":{"id":"YSy8mAy-jjNm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["whether adding previous volatility values or not:"],"metadata":{"id":"EWxKkLM-jnBx"}},{"cell_type":"code","source":["import numpy as np\n","df2['features']=\"\"\n","for i in range(len(df2)):  \n","  count = 0\n","  my_list = df2.loc[i,['tf']].tf.split(\", \") #df2.loc[i,['tfidf']].tfidf.split(\", \") \n","  if '[' in my_list:\n","    my_list.remove('[')\n","  if ']' in my_list:\n","    my_list.remove(']')\n","  for x in my_list:\n","    if my_list[count]!='[':\n","      if count==0:\n","        my_list[count]=my_list[count][1:]\n","      if count==len(my_list)-1:\n","        my_list[count]=my_list[count][:-1]\n","      my_list[count] = float(my_list[count])\n","      count+=1\n","      \n","  my_list=np.array(my_list)\n","  overall =np.hstack((my_list,df3.loc[i,['before']].before))\n","  df2.at[i,'features']= overall#my_list"],"metadata":{"id":"MJJ6c4BEjr7d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["relevant year interval arrangement is made for the code below"],"metadata":{"id":"KlcCggdTkPF3"}},{"cell_type":"code","source":["from sklearn.linear_model import LinearRegression\n","import numpy as np\n","X_train = np.vstack(df2[df2['year'].isin([2002,2003,2001,2000,2004])]['features'].to_numpy()) #.isin([1998,1997,1996,2000,1999])]['features'].to_numpy())\n","y_train = df2[df2['year'].isin([2002,2003,2001,2000,2004])]['real'].to_numpy()\n","X_test  = np.vstack(df2[df2['year']==2005]['features'].to_numpy())\n","y_test = df2[df2['year']==2005]['real'].to_numpy()"],"metadata":{"id":"I35TmwmFj8rf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.svm import SVR\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","regr = make_pipeline(StandardScaler(), SVR())#(C=1.0, epsilon=0.2))\n","regr.fit(X_train,y_train)\n","y_pred= regr.predict(X_test)"],"metadata":{"id":"D4AFnlSCkcGu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import mean_squared_error\n","mean_squared_error(y_test,y_pred)"],"metadata":{"id":"cQM3kXWHkeKF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# bert-based embeddings"],"metadata":{"id":"l00M55nhnDcs"}},{"cell_type":"markdown","source":["relevant year interval arrangement is made for the code below"],"metadata":{"id":"6j-SeV8XngrS"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.datasets import make_classification\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import mean_squared_error\n","from xgboost import XGBRegressor\n","df3 = used_df\n","from sklearn.linear_model import LinearRegression\n","import numpy as np\n","from sklearn.svm import SVR\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","for i in range(2002,2017):\n","\tprint(i,'-',i+4,'\\n')\n","\tX_train = np.vstack(df3[df3['year'].isin([i,i+1,i+2,i+3,i+4])]['features'].to_numpy()) #.isin([1998,1997,1996,2000,1999])]['features'].to_numpy())\n","\ty_train = df3[df3['year'].isin([i,i+1,i+2,i+3,i+4])]['real'].to_numpy()\n","\tX_test  = np.vstack(df3[df3['year']==i+5]['features'].to_numpy())\n","\ty_test = df3[df3['year']==i+5]['real'].to_numpy()\n","\t#RANDOM FOREST  \n","\tprint('random forest\\n')\n","\trf = RandomForestRegressor()\n","\trf.fit(X_train,y_train)\n","\ty_pred= rf.predict(X_test)\n","\trs = mean_squared_error(y_test,y_pred)\n","\tprint('result: ',rs,'\\n' )\n","\t\n","\t\n","\tprint('XGBRegressor\\n')\n","\tmodel = XGBRegressor(n_estimators=100,objective='reg:squarederror')\n","\tmodel.fit(X_train, y_train)\n","\ty_pred = model.predict(X_test)\n","\trs = mean_squared_error(y_test,y_pred)\n","\tprint('result: ',rs,'\\n' )\n","\t\n","\tprint('SVR \\n')\n","\tregr = make_pipeline(StandardScaler(), SVR())#(C=1.0, epsilon=0.2))\n","\tregr.fit(X_train,y_train)\n","\ty_pred= regr.predict(X_test)\n","\trs = mean_squared_error(y_test,y_pred)\n","\tprint('result: ',rs,'\\n' )"],"metadata":{"id":"1cgqtH7KnL_y"},"execution_count":null,"outputs":[]}]}